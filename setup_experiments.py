#!/usr/bin/env python3
"""
Quick setup script for OOD experiments
Downloads datasets and sets up the environment
"""

import os
import torch
import torchvision
from pathlib import Path
import argparse
from utils.datasets import DatasetManager

def download_datasets(data_dir="./data"):
    """Download and verify required datasets."""
    data_path = Path(data_dir)
    data_path.mkdir(exist_ok=True)
    
    print("üîΩ Downloading datasets...")
    
    # CIFAR-10 (already available in training)
    print("   üìÅ CIFAR-10...")
    try:
        torchvision.datasets.CIFAR10(root=data_path, train=False, download=True)
        print("      ‚úÖ CIFAR-10 ready")
    except Exception as e:
        print(f"      ‚ùå CIFAR-10 failed: {e}")
    
    # SVHN
    print("   üìÅ SVHN...")
    try:
        torchvision.datasets.SVHN(root=data_path, split='test', download=True)
        print("      ‚úÖ SVHN ready")
    except Exception as e:
        print(f"      ‚ùå SVHN failed: {e}")
    
    print("‚úÖ Dataset download complete")

def verify_model_files():
    """Check if trained model files exist."""
    print("\nüîç Checking for trained models...")
    
    model_files = [
        "checkpoints/resnet_final.pth",
        "checkpoints/vit_final.pth",
        "best_resnet_model.pth",
        "best_vit_model.pth"
    ]
    
    found_models = []
    for model_file in model_files:
        if Path(model_file).exists():
            found_models.append(model_file)
            print(f"      ‚úÖ Found: {model_file}")
    
    if not found_models:
        print("      ‚ö†Ô∏è  No trained models found. Please train models first:")
        print("         python main.py --model resnet --epochs 50")
        print("         python main.py --model vit --epochs 50")
    else:
        print(f"‚úÖ Found {len(found_models)} trained model(s)")
    
    return found_models

def create_experiment_commands(model_files):
    """Generate ready-to-run experiment commands."""
    commands_file = Path("experiment_commands.txt")
    
    commands = []
    
    for model_file in model_files:
        model_type = "resnet" if "resnet" in model_file.lower() else "vit"
        
        # Full experiment
        commands.append(f"# Full {model_type.upper()} OOD experiment")
        commands.append(f"python run_ood_experiments.py --model_path {model_file} --model_type {model_type} --output_dir results/{model_type}")
        commands.append("")
        
        # Quick test
        commands.append(f"# Quick {model_type.upper()} test (for debugging)")
        commands.append(f"python run_ood_experiments.py --model_path {model_file} --model_type {model_type} --output_dir results/{model_type}_quick --quick_test")
        commands.append("")
    
    # Add comparison command
    if len(model_files) >= 2:
        commands.append("# Compare results after both experiments")
        commands.append("python compare_ood_results.py --results_dir results/")
        commands.append("")
    
    with open(commands_file, 'w') as f:
        f.write('\n'.join(commands))
    
    print(f"üìù Experiment commands saved to: {commands_file}")
    return commands_file

def test_attribution_methods():
    """Quick test of attribution methods."""
    print("\nüß™ Testing attribution methods...")
    
    try:
        from utils.attribution_methods import AttributionSuite
        from models.resnet import ResNet18
        
        # Create dummy model and input
        model = ResNet18(num_classes=10)
        suite = AttributionSuite(model, 'resnet')
        
        # Test with dummy input
        dummy_input = torch.randn(2, 3, 32, 32)
        
        # Test saliency
        try:
            saliency, _ = suite.saliency.generate(dummy_input)
            print("      ‚úÖ Saliency maps working")
        except Exception as e:
            print(f"      ‚ùå Saliency failed: {e}")
        
        # Test simple model sanity check
        simple_model = torch.nn.Sequential(
            torch.nn.Flatten(),
            torch.nn.Linear(3*32*32, 10)
        )
        
        sanity_results = suite.run_all_sanity_checks(simple_model)
        if sanity_results:
            print("      ‚úÖ Sanity checks working")
        else:
            print("      ‚ö†Ô∏è  Sanity checks returned empty")
            
    except Exception as e:
        print(f"      ‚ùå Attribution test failed: {e}")

def main():
    parser = argparse.ArgumentParser(description='Setup OOD experiments')
    parser.add_argument('--data_dir', type=str, default='./data', help='Data directory')
    parser.add_argument('--skip_download', action='store_true', help='Skip dataset download')
    parser.add_argument('--test_only', action='store_true', help='Only run tests')
    args = parser.parse_args()
    
    print("üöÄ Setting up OOD experiments")
    print("=" * 50)
    
    if not args.test_only:
        # Download datasets
        if not args.skip_download:
            download_datasets(args.data_dir)
        
        # Check models
        model_files = verify_model_files()
        
        # Create experiment commands
        if model_files:
            create_experiment_commands(model_files)
    
    # Test attribution methods
    test_attribution_methods()
    
    print("\n‚úÖ Setup complete!")
    
    if not args.test_only:
        print("\nNext steps:")
        print("1. Check experiment_commands.txt for ready-to-run commands")
        print("2. Start with a quick test: --quick_test flag")
        print("3. Run full experiments when ready")
        print("4. Results will be saved in results/ directory")

if __name__ == '__main__':
    main() 